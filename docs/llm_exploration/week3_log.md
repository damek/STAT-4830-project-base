# LLM Exploration Summary

## Session Focus
Main thing was trying to figure out how to initially implement the model and what to focus on.

## Surprising Insights

### Conversation: [Brief Topic]
**Prompt That Worked:** 
- Make sure to attach our initial report so the LLM has a very good overview of what we're trying to do.

**Key Insights:**
- Try using synthetic data at first. There may or may not be a difference (ending up being right so far)
- You can actually train a transformer model on my own laptop!

## Techniques That Worked
- Again attached essentially all the content from the initial report. We can use this to help us understand what we're doing and what we should be doing. Also making new conversations frequently so
it didn't feel like we were just repeating ourselves.

## Dead Ends Worth Noting
- No dead ends yet!

## Next Steps
- Hyperparameter tuning
- More advanced transformer architectures (will need to look at research)
- Encoding the structure of the sequence (will need to look at research)

